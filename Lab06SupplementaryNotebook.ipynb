{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTpqom9vQKXF"
      },
      "source": [
        "# Lab 6:  EDA using a massive airline dataset\n",
        "\n",
        "For this lab, you'll need to access a 500MB+ dataset at kaggle.com.  First, register at kaggle.com, login and download the dataset at https://www.kaggle.com/usdot/flight-delays.  What you'll actually download is a 200MB `.zip` file, which you should save somewhere on your computer.\n",
        "\n",
        "Next, visit https://drive.google.com/drive/my-drive and create a new folder\n",
        "using the \"New\" button where your datasets will go.  On my google drive space,\n",
        "I created a folder called \"data\" inside the \"Colab Notebooks\" folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload archive.zip\n",
        "\n",
        "After downloading `archive.zip` from Kaggle, I then uploaded the same file\n",
        "directly into my `drive/My Drive/Colab Notebooks/data/` folder that I had created as described above.  Then I could extract, or unzip, the file, using the ZIP extractor.\n",
        "\n",
        "Alternatively, you might prefer to unzip\n",
        "`archive.zip` on your own computer before uploading the three resulting .csv files to your google drive space.  It's up to you, as long as you wind up with `airlines.csv`, `airports.csv`, and `flights.csv` in a google drive folder you can access."
      ],
      "metadata": {
        "id": "yOuBfbcXBz_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount the google drive\n",
        "\n",
        "After your three .csv files are in place, you have to make sure that your Jupyter notebook can access them from the google.colab environment.  That's what the code above does after you run it, by mounting the drive.  You can actually get the colab to insert this code block automatically if you click the folder icon along the left side of the screen and then click the \"mount drive\" icon that appears at the top of the left margin."
      ],
      "metadata": {
        "id": "qLBVGp-rbWwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dbVzDcfcGs7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After your three .csv files are in place, you have to make sure that your Jupyter notebook can access them from the google.colab environment.  That's what the code above does after you run it, by mounting the drive.  You can actually get the colab to insert this code block automatically if you click the folder icon along the left side of the screen and then click the \"mount drive\" icon that appears at the top of the left margin."
      ],
      "metadata": {
        "id": "4FB9zHBnGzIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change to the correct working directory\n",
        "\n",
        "Next, we can change the working directory (folder) in which the Jupyter notebook looks for files.  First, let's see what the current working directory is using `pwd` (print working directory)."
      ],
      "metadata": {
        "id": "3vnegt8zJ4Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "x-ZRolclMxef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `cd` (change directory) to change the working directory to a different folder.  A space has to be entered as `\\ ` (backslash space)."
      ],
      "metadata": {
        "id": "wDfvfzcoNGCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab\\ Notebooks/data"
      ],
      "metadata": {
        "id": "4NE932imLTF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can verify that the three `.csv` files are in the new working directory using `ls` (list)."
      ],
      "metadata": {
        "id": "VxKu1ThGNclT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "zVnpxPuRMRaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNPXcT--QfOO"
      },
      "source": [
        "# Load the datascience library and other resources\n",
        "\n",
        "Once all the data files are in place, we can download and analyze the data.  The first step, as usual, is to load the necessary python resources. The code below is written to use the `datascience` library rather than `pandas`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5kgObjIQJXU"
      },
      "source": [
        "# Load needed python resources\n",
        "from datascience import *\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "matplotlib.use('Agg')\n",
        "%matplotlib inline\n",
        "import math\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import warnings\n",
        "np.set_printoptions(threshold=50)\n",
        "warnings.simplefilter(action='ignore', category=np.VisibleDeprecationWarning)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeyRsOFMQjVG"
      },
      "source": [
        "Next, we'll read the largest of the three files as a `Table` object.  This may take a bit of time to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATJPJK6JTMzi"
      },
      "source": [
        "# This code only works if flights.csv is in the current working directory (see above)\n",
        "flights = Table.read_table('flights.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrK6aMaNTQUR"
      },
      "source": [
        "Let's check out the columns available in the flights dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iX8Xi30TNBp"
      },
      "source": [
        "flights.labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNiM6kg8TXa2"
      },
      "source": [
        "To create a more manageable dataset, select a systematic sample of rows from the flights Table object.  See if you can determine a good value for `gap` if you want to keep every `gap` row, starting from a randomly selected row near the beginning.  You should aim for 500 to 1000 rows in your sample chosen from among the total number of rows in the `flights` dataset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flights.num_rows"
      ],
      "metadata": {
        "id": "IN5uLIJpghY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl9XgT8rTXyQ"
      },
      "source": [
        "gap = ***ENTER AN APPROPRIATE VALUE HERE***\n",
        "start = np.random.choice(np.arange(gap))\n",
        "mySample = flights.take(np.arange(start, flights.num_rows, gap))\n",
        "print(mySample.num_rows)\n",
        "mySample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-sxSL1ITX_e"
      },
      "source": [
        "Load the airports dataset containing airport names and latitude/longitude coordinates:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LgSYfXbTYR_"
      },
      "source": [
        "airports = Table.read_table('airports.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTdcE9nATYeP"
      },
      "source": [
        "## Use the join method to add airport information\n",
        "\n",
        "Notice how the following code block is formatted so that it strings together multiple Table-modifying methods (from the `datascience` library), where each method produces a new Table as output that feeds into the next method, to produce a cleaned-up version of the systematic sample called mySample (then print its number of rows and its first 10 rows):\n",
        "\n",
        "1.   Select mySample according to 'gap' and 'start'.\n",
        "2.   Then join every ORIGIN_AIRPORT in 'mysample' with the\n",
        " corresponding columns from 'airports' based on matching IATA_CODE.\n",
        "3.   Then select just the columns we need from the result.\n",
        "4.   Then relabel some of the columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T10NXfo1TYue"
      },
      "source": [
        "mySample = (flights.take(np.arange(start, flights.num_rows, gap))\n",
        "                   .join('ORIGIN_AIRPORT', airports, 'IATA_CODE')\n",
        "                   .select('MONTH', 'DAY', 'ORIGIN_AIRPORT',\n",
        "                           'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE',\n",
        "                           'DEPARTURE_DELAY', 'AIRPORT',\n",
        "                           'LATITUDE', 'LONGITUDE')\n",
        "                   .relabeled('ORIGIN_AIRPORT', 'ORIGIN')\n",
        "                   .relabeled('DESTINATION_AIRPORT', 'DESTINATION')\n",
        "                   .relabeled('DEPARTURE_DELAY', 'DELAY')\n",
        "                   .relabeled('AIRPORT', 'ORIGIN_NAME')\n",
        "            )\n",
        "print(mySample.num_rows)\n",
        "mySample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoALcujETY72"
      },
      "source": [
        "Create a new column with the approximate day of year. There are better, more accurate ways to do this, but this method that approximates each month by 30 days will work for this purpose:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5tM0e-3UeFq"
      },
      "source": [
        "mySample = mySample.with_column(\n",
        "             'APPROX_DAY_OF_YEAR',\n",
        "             30*(mySample.column('MONTH')-1) + mySample.column('DAY'))\n",
        "mySample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrzgL6WJUeua"
      },
      "source": [
        "Recall that a scatterplot depicts the relationship between two quantitative measurement columns.  Create a scatterplot using the 'LATITUDE' and 'DELAY' columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR1ck_htUfx4"
      },
      "source": [
        "mySample.scatter('LATITUDE', 'DELAY')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAblkOH4Uf-w"
      },
      "source": [
        "# EXTRA CODE THAT YOU MIGHT FIND HELPFUL\n",
        "\n",
        "The code below is not strictly necessary for the lab assignment.  It is included, with comments to help you understand what it does, to illustrate some potentially interesting directions you could take your own investigation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VvJn0PFUgUo"
      },
      "source": [
        "# Create a histogram of LATITUDE\n",
        "mySample.hist('LATITUDE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column that splits the airports into high vs. low latitude\n",
        "# based on a cutoff you define:\n",
        "LatCut = **ENTER A VALUE THAT MAKES SENSE HERE**\n",
        "mySample = mySample.with_column(\n",
        "              'HIGH_LAT', mySample.column('LATITUDE') > LatCut)\n",
        "mySample"
      ],
      "metadata": {
        "id": "kNs44dU8S9kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Figure out how many airports are \"high latitude\" vs. \"low latitude\"\n",
        "mySample.group('HIGH_LAT')"
      ],
      "metadata": {
        "id": "0U1Sj3jwS3Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the means for the high- and low- latitude airports.\n",
        "# We're using the 'nanmean' method in numpy to ignore the\n",
        "# nan (not a number) values\n",
        "mySample.group('HIGH_LAT', np.nanmean)"
      ],
      "metadata": {
        "id": "wbE_4pZGS0c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the previous idea to find the mean difference automatically\n",
        "Observed_mean_difference = np.diff(mySample.group('HIGH_LAT', np.nanmean)\n",
        "                                           .column('DELAY nanmean'))[0]\n",
        "Observed_mean_difference"
      ],
      "metadata": {
        "id": "QDWMgdC-Sx-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that will reshuffle the DELAY values and then return\n",
        "# the mean difference statistic for the shuffled table.\n",
        "# This simulates from the null hypothesis distribution of the\n",
        "# mean difference statistic.\n",
        "def simulated_mean_difference_under_null():\n",
        "    a=(mySample.sample(with_replacement=False)\n",
        "               .select('DELAY')\n",
        "               .with_column('HIGH_LAT', mySample.column('HIGH_LAT')))\n",
        "    return (np.diff(a.group('HIGH_LAT', np.nanmean)\n",
        "                     .column('DELAY nanmean'))[0])"
      ],
      "metadata": {
        "id": "A3JJGGMISvbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate 5000 draws from the null hypothesis distribution of the\n",
        "# mean difference (and return the result as a numpy array--not the same\n",
        "# as a datascience Table)\n",
        "H0_means = make_array()\n",
        "for i in np.arange(5000):\n",
        "    H0_means = np.append(H0_means, simulated_mean_difference_under_null())"
      ],
      "metadata": {
        "id": "yWQphnc6Spov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a table with the 5000 H0 (null hypothesis) values and then\n",
        "# create a histogram.\n",
        "# Also add the observed value of the sample statistic as a red dot\n",
        "# along the x-axis.\n",
        "Table().with_column(\n",
        "    'Count in a Random Sample', H0_means\n",
        ").hist(bins = np.arange(-12.5, 12.5, 1))\n",
        "plots.scatter(Observed_mean_difference, 0, color='red', s=30);"
      ],
      "metadata": {
        "id": "dbbqzOmDSmH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}